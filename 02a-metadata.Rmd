# FAIR Data and the Added Value of Rich Metadata {#FAIR-data}

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 50%" />
<col style="width: 25%" />
</colgroup>
<tbody>
<tr class="even">
<td style="text-align: center;  vertical-align: top;"></td>
<td style="text-align: center;  vertical-align: top;"><img src="plots/gems/Diamond_Polisher.jpg" /><a href="https://contributors.dataobservatory.eu/FAIR-data.html">Adding metadata exponentially increases the value of data</a></br>Did your region added a new town to its boundaries – how do you adjust old data? Can I practically combine satellite sensory data with my organization records? And do I have the right? Metadata logs the history of the data, gives instructions who to reuse it in indicators, sets the terms of use. We automate this boring and labor-intensive process applying the [FAIR](https://contributors.dataobservatory.eu/FAIR-data.html#FAIR) data concept. We provide you with polished data, or we process your uncut dataset diamonds to shape.</br></td>
<td style="text-align: center;  vertical-align: top;"></td>
</tr>
</tbody>
</table>

We are providing a comprehensive, automated solution to problems that are not so much related to the data itself, but the information, documentation of the data, i.e. the metadata, and the structure how the data is stored. These problems create countless non-billable hours in professional services, or daunting extra work in research institutions that do not result in credited publications and other research output. These problems should be eliminated or handled by computers. We believe that the most value in our research automation comes from the documentation automation, which adds automatically rich descriptive, administrative and processing metadata to your data assets.

**Lack of clarity with respect to:**
- The legal consequences of reusing a dataset. Do licensing restriction apply?  What are the correct attributions to creators, contributors, publishers?

- Who checked the reliability of the dataset? Did it change?

- It is unclear what is the content of a data file.  Who used it, who controlled it, does it need further work? The description is missing or difficult to use. There are no systematic keywords applied to the files, data, and it is impossible to run a search on your computer that will deliver the right asset.

- Is this the latest verion of the dataset?  Is there a new version available somewhere? Did somebody recast the data, did corrections take place?  This can be particularly labor intensive to check if you use external data sources. 

**The lack of reusability:**
- If you open a dataset, you need to make several moves with your mouse or apply several keystrokes (all error-prone and generate new supervisory, control taks) before you can start analyzing or visualizting the data. Generally, as soon as you need to use a mouse, plenty of new work and cost is generated.

- What are the units or currency rates used? Is the data expressed in euros, milions of euros? Do you need to translate dollars to euros on the average rate, the spot rate?  What is the functional currency of the problem?

- The data cannot be easily imported, further processing is needed before you can add it a relational database. Tidy data can be imported without further work.

- The data has many versions in your organization.

- Corrections were not logged, and before using it or handing it over to a client, all verification processes must be repeated, even though they may have been done several times in the past.

**Potential mistakes during use**:
- Wrong currency translations, or wrong type of financial data merged. 

- Annual, year end data divided by annual, middle of the year data.

- Accidentally units recorded in thousands of euros and euros are mixed up. 

- During tidying a digit is entered accidentally to the keyboard. Or a field is erased.

Some of these problems are controled by database schemas, but database schemas are rigid and most workers do not like to work with them. We use different schemas that are not as strict as schemas applied in databases, and we use automation software that applies these schemas to documents and data has been created by your colleagues in the past without reference to schemas. We apply instead the FAIR Data concept stands for **f**indable, **a**ccessibe, **i**nteroperable, and **r**eusable of digital assets, for example, when we re-create a missing codebook to a survey documentation that was carried out years ago.

The problem with [Open Data](#open-data) is that while it is legally *accessible*, and often cost-free, it in most cases *not findable*, and not even accessible directly. While the EU has policies since 2003 about making taxpayer funded data reusable, it did not make much technical steps to make this a reality.  Reusability of governmental data and scientific data is a right, but not a practical possiblity for most users. The data may sit in various historical file formats, without documentation.

Interoperability means that you can use governmental open data, scientific open data, your own system's data, your membership organizations shared resources, and data subscriptions together. A special case of lack of interoperability when you do not know if you are facing a legal risk from using a data resource.

In our experience, in most research-driven organizations, such as consultancies, law firms, university research groups, NGOs and public policy bodies, reusability is mainly limited by poor data documentation, and sometimes by the use of of obsolete proprietary file formats. Documentation in the short run is not always a necessity, and it belongs to the non-billable hours, or among the tasks that do not really count in a researchers' promotion. The poor documentation however makes it extremely demanding to re-use a data or document a few years from now. From 2021, if you apply for Horizon Europe funding, your research output must meet basic findability and reusability criteria. 

- Our data comes with metadata that meets the requirements of two metadata standards, the more general [Dublin Core](#Dublin-Core) and the more specific, mandatory and recommended values of [DataCite](#DataCite) for datasets. We go even further, we add rich processing metadata beyond these requirements. These are not only nice to have: from 2021, if you apply for Our solution can automate this process, and besides making you compliant, it adds a lot of value to your own data management.

- We are making our data-as-service APIs FAIR by automatically adding to our data standardized metadata that fulfills the mandatory requirements of the Dublic Core metadata standards and at the same time the [mandatory requirements](https://support.datacite.org/docs/datacite-metadata-schema-v44-mandatory-properties), and most of the [recommended requirements](https://support.datacite.org/docs/datacite-metadata-schema-v44-recommended-and-optional-properties) of DataCite.

- Furthermore, we apply the [tidy data](#tidy-data) concept to our partners data assets. The tidy data principle applies a certain structure to datasets that facilitates immediate use and reuse. 

## FAIR {#FAIR}

In 2016, the **[FAIR Guiding Principles for scientific data management and stewardship](http://www.nature.com/articles/sdata201618)** were published in _Scientific Data_. The authors intended to provide guidelines to improve the **F**indability, **A**ccessibility, **I**nteroperability, and **R**euse of digital assets. The principles emphasize machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data because of the increase in volume, complexity, and creation speed of data.

A practical “how to” guidance to go FAIR can be found in the **[Three-point FAIRification Framework](https://www.go-fair.org/how-to-go-fair/)**.

**<span style="text-decoration:underline;">F</span>indable**

The first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services, so this is an essential component of the **[FAIRification process](https://www.go-fair.org/fair-principles/fairification-process/)**.

**[F1. (Meta)data are assigned a globally unique and persistent identifier](https://www.go-fair.org/fair-principles/fair-data-principles-explained/f1-meta-data-assigned-globally-unique-persistent-identifiers/)**

**[F2. Data are described with rich metadata (defined by R1 below)](https://www.go-fair.org/fair-principles/fair-data-principles-explained/f2-data-described-rich-metadata/)**

**[F3. Metadata clearly and explicitly include the identifier of the data they describe](https://www.go-fair.org/fair-principles/f3-metadata-clearly-explicitly-include-identifier-data-describe/)**

**[F4. (Meta)data are registered or indexed in a searchable resource](https://www.go-fair.org/fair-principles/f4-metadata-registered-indexed-searchable-resource/)**

**<span style="text-decoration:underline;">A</span>ccessible**

Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation.

**[A1. (Meta)data are retrievable by their identifier using a standardised communications protocol](https://www.go-fair.org/fair-principles/542-2/)**

**[A1.1 The protocol is open, free, and universally implementable](https://www.go-fair.org/fair-principles/a1-1-protocol-open-free-universally-implementable/)**

**[A1.2 The protocol allows for an authentication and authorisation procedure, where necessary](https://www.go-fair.org/fair-principles/a1-2-protocol-allows-authentication-authorisation-required/)**

**[A2. Metadata are accessible, even when the data are no longer available](https://www.go-fair.org/fair-principles/a2-metadata-accessible-even-data-no-longer-available/)**

**<span style="text-decoration:underline;">I</span>nteroperable**

The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.

**[I1. (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.](https://www.go-fair.org/fair-principles/i1-metadata-use-formal-accessible-shared-broadly-applicable-language-knowledge-representation/)**

**[I2. (Meta)data use vocabularies that follow FAIR principles](https://www.go-fair.org/fair-principles/i2-metadata-use-vocabularies-follow-fair-principles/)**

**[I3. (Meta)data include qualified references to other (meta)data](https://www.go-fair.org/fair-principles/i3-metadata-include-qualified-references-metadata/)**

**<span style="text-decoration:underline;">R</span>eusable**

The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.

**[R1. (Meta)data are richly described with a plurality of accurate and relevant attributes](https://www.go-fair.org/fair-principles/r1-metadata-richly-described-plurality-accurate-relevant-attributes/)**

**[R1.1. (Meta)data are released with a clear and accessible data usage license](https://www.go-fair.org/fair-principles/r1-1-metadata-released-clear-accessible-data-usage-license/)**

**[R1.2. (Meta)data are associated with detailed provenance](https://www.go-fair.org/fair-principles/r1-2-metadata-associated-detailed-provenance/)**

**[R1.3. (Meta)data meet domain-relevant community standards](https://www.go-fair.org/fair-principles/r1-3-metadata-meet-domain-relevant-community-standards/)**

The principles refer to three types of entities: data (or any digital object), metadata (information about that digital object), and infrastructure. For instance, principle F4 defines that both metadata and data are registered or indexed in a searchable resource (the infrastructure component).


## The Dublin Core {#Dublin-Core}

See: [Dublin Core Cross-Domain Attribute Set](https://www.dublincore.org/specifications/dublin-core/cross-domain-attribute/)

|   |   |   
|:--|:-:|
| Contributor | An entity responsible for making contributions to the resource.|
|Coverage |The spatial or temporal topic of the resource, the spatial applicability of the resource, or the jurisdiction under which the resource is relevant. |
|Creator | An entity primarily responsible for making the resource.|
|Date | A point or period of time associated with an event in the lifecycle of the resource.|
|Description |An account of the resource.|
|Format |The file format, physical medium, or dimensions of the resource.|
|Identifier | An unambiguous reference to the resource within a given context.|
|Language | A language of the resource.|
|Publisher | An entity responsible for making the resource available. |
|Relation | A related resource. |
|Rights | Information about rights held in and over the resource.|
|Source | A related resource from which the described resource is derived.|
|Subject| The topic of the resource.|
|Title |A name given to the resource.|
|Type |The nature or genre of the resource.|

## DataCite  {#DataCite}

We use all [mandatory](https://support.datacite.org/docs/datacite-metadata-schema-v44-mandatory-properties) DataCite metadata fields, and many of [the recommended and optional](https://support.datacite.org/docs/datacite-metadata-schema-v44-recommended-and-optional-properties) ones. 

See for further reference [DataCite Descriptive Metadata](https://r.dataobservatory.eu/articles/datacite.html).

|   |   |   
|:--|:-:|
|Identifier | An unambiguous reference to the resource within a given context. (Dublin Core item), but several identifiders allowed, and we will use several of them.|
|Creator | The main researchers involved in producing the data, or the authors of the publication, in priority order. To supply multiple creators, repeat this property. (Extends the Dublin Core with multiple authors, and legal persons, and adds affiliation data.) |
|Title |A name given to the resource. Extends Dublin Core with alternative title, subtitle, translated Title, and other title(s). |
|Publisher | The name of the entity that holds, archives, publishes prints, distributes, releases, issues, or produces the resource. This property will be used to formulate the citation, so consider the prominence of the role. For software, use Publisher for the code repository. (Dublin Core item.) |
|Publication Year | The year when the data was or will be made publicly available. |
|Resource Type |We publish Datasets, Images, Report, and Data Papers. (Dublin Core item with controlled vocabulary.)|

### Recommended for discovery

|   |   |   
|:--|:-:|
|Subject| The topic of the resource. (Dublin Core item.)|
|Contributor | The institution or person responsible for collecting, managing, distributing, or otherwise contributing to the development of the resource. (Extends the Dublin Core with multiple authors, and legal persons, and adds affiliation data.) When applicable, we add Distributor (of the datasets and images), Contact Person, Data Collector, Data Curator, Data Manager, Hosting Institution, Producer (for images), Project Manager, Researcher, Research Group, Rightsholder, Sponsor, Supervisor |
|Date | A point or period of time associated with an event in the lifecycle of the resource, besides the Dublin Core minimum we add Collected, Created, Issued, Updated, and if necessary, Withdrawn dates to our datasets.|
|Related Identifier |An identifier or identifiers other than the primary Identifier applied to the resource being registered. |
|Rights | We give [SPDX License List](https://spdx.org/licenses/) standards rights description with URLs to the actual license. (Dublin Core item: Rights Management)|
|Description | Recommended for discovery.(Dublin Core item.) |
|GeoLocation | Similar to Dublin Core item Coverage |
|Related Item |We give information about our observatory partners' related research products, awards, grants (also Dublin Core item as Relation.) We particularly include source information when the dataset is derived from another resource (which is a Dublin Core item.)|


### Optional

|   |   |   
|:--|:-:|
|Language | A language of the resource. (Dublin Core item.)|
|Alternative Identifier |An identifier or identifiers other than the primary Identifier applied to the resource being registered. |
|Size |We give the CSV, downloadable dataset size in bytes. |
|Format |We give file format information. We mainly use CSV and JSON, and occasionally rds and SPSS types. (Dublin Core item.) |
|Version | The version number of the resource.  |
|Rights | We give [SPDX License List](https://spdx.org/licenses/) standards rights description with URLs to the actual license. (Dublin Core item: Rights Management)|
|Funding Reference |We provide the funding reference information when applicable. This is usually mandatory with public funds. |
|Related Item |We give information about our observatory partners' related research products, awards, grants (also Dublin Core item as Relation.) We particularly include source information when the dataset is derived from another resource (which is a Dublin Core item.)|


## Administrative Metadata

Like with diamonds, it is better to know the history of a dataset, too. Our administrative metadata contains codelists that follow the SXDX statistical metadata standards, and similarly strucutred information about the processing history of the dataset.

See for further reference [The codebook Class](https://r.dataobservatory.eu/articles/codebook.html).

|   |   |   
|:--|:-:|
|Observation Status| SDMX Code list for [Observation Status 2.2](https://sdmx.org/?sdmx_news=new-version-of-code-list-for-observation-status-version-2-2) (CL_OBS_STATUS), such as actual, missing, imputed, etc. values. |
|Method| If the value is estimated, we provide modelling information.|
|Unit| We provide the measurement unit of the data (when applicable.)|
|Frequency| [SDMX Code list for Frequency 2.1 (CL_FREQ)](https://sdmx.org/?page_id=3215/) frequency values |
|Codelist| Euros-SDMX Codelist entries for the observational units, such as sex, etc. |
|Imputation| SDMX Code list for Frequency 2.1 (CL_IMPUT_METH) imputation values |
|Estimation| The estimation methodology of data that we calculated, together with citation information and URI to the actual processing code |
|Related Item |We give information about the software code that processed the data (both Dublin Core and DataCite compliant.)|


## Tidy Data {#tidy-data}

A dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.

It is often said that 80% of data analysis is spent on the cleaning and preparing data. And it’s not just a first step, but it must be repeated many times over the course of analysis as new problems come to light or new data is collected. The tidy data principle applies a certain structure to datasets that facilitates immediate use and reuse. 

The principles of tidy data provide a standard way to organise data values within a dataset. A standard makes initial data cleaning easier because you don’t need to start from scratch and reinvent the wheel every time. 

Tidy data is a standard way of mapping the meaning of a dataset to its structure (. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. 

In tidy data we need a way to describe the underlying semantics, or meaning, of the values displayed in the table:

- Every column is a variable.
- Every row is an observation.
- Every cell is a single value.

## Messy data {#messy-data}

Real datasets can, and often do, violate the three precepts of tidy data in almost every way imaginable -- this particulary true of open data, even when it is released by statistical bodies. The most typical errors:

* Column headers are values, not variable names.
* Multiple variables are stored in one column, for example, the number of item purchased and the price of the item.
* Variables are stored in both rows and columns, for example, the columns are organized by income level groups. 
* Multiple types of observational units are stored in the same table. For example, names of musicians and their songs.
* A single observational unit is stored in multiple tables.

While messy data almost always can be tidied, if you do this in a spreadsheet application manually, it is almost impossible not to make a mistake.  Spreadsheet applications do not check the tidyness of the data, and do not record the logs of your manual tidying efforts.  Unless every single mouseclick, drag and drop is recorded, the work is impossible to validate. However, we believe that data should never be manipulated with a mouse.  Computer algorithms should be deployed in a way that our their tidying efforts are reproducible and logged.
